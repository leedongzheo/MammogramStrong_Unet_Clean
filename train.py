import argparse
from dataset import*

def get_args():
    # Tham s·ªë b·∫Øt bu·ªôc nh·∫≠p
    parser = argparse.ArgumentParser(description="Train, Pretrain ho·∫∑c Evaluate m·ªôt model AI")
    parser.add_argument("--epoch", type=int, help="S·ªë epoch ƒë·ªÉ train")
    # parser.add_argument("--model", type=str, required=True, help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn model")
    parser.add_argument("--mode", type=str, choices=["train", "pretrain", "evaluate"], required=True, help="Ch·∫ø ƒë·ªô: train ho·∫∑c pretrain ho·∫∑c evaluate")
    parser.add_argument("--data", type=str, required=True, help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn dataset ƒë√£ gi·∫£i n√©n")
    # Tham s·ªë tr∆∞·ªùng h·ª£p
    parser.add_argument("--checkpoint", type=str, help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn file checkpoint (ch·ªâ d√πng cho ch·∫ø ƒë·ªô pretrain)")
    parser.add_argument("--augment", action='store_true', help="B·∫≠t Augmentation cho d·ªØ li·ªáu ƒë·∫ßu v√†o")
    # Tham s·ªë m·∫∑c ƒë·ªãnh(default)
    parser.add_argument("--saveas", type=str, help="Th∆∞ m·ª•c l∆∞u checkpoint")
    parser.add_argument("--lr0", type=float, help="learning rate, default = 0.0001")
    parser.add_argument("--batchsize", type=int, help="Batch size, default = 8")

    parser.add_argument("--weight_decay", type=float,  help="weight_decay, default = 1e-6")
    parser.add_argument("--img_size", type=int, nargs=2,  help="Height and width of the image, default = [256, 256]")
    parser.add_argument("--numclass", type=int, help="shape of class, default = 1")
    parser.add_argument("--warmup", type=int, default=10, help="S·ªë epoch ƒë·ªÉ warm-up (augment nh·∫π)")
    """
    # V·ªõi img_size, c√°ch ch·∫°y: python script.py --img_size 256 256
    N·∫øu mu·ªën nh·∫≠p list d√†i h∆°n 3 ph·∫ßn t·ª≠, g√µ 
    parser.add_argument("--img_size", type=int, nargs='+', default=[256, 256], help="Image dimensions")
    Ch·∫°y:
    python script.py --img_size 128 128 3
    """
    parser.add_argument("--loss", type=str, choices=["Dice_loss", "Hybric_loss", "BCEDice_loss", "BCEwDice_loss", "BCEw_loss", "SoftDice_loss", "Combo_loss", "Tversky_loss", "FocalTversky_loss" ], default="Combo_loss", help="H√†m loss s·ª≠ d·ª•ng, default = Combo_loss")
    parser.add_argument("--optimizer", type=str, choices=["Adam", "SGD", "AdamW"], default="AdamW", help="Optimizer s·ª≠ d·ª•ng, default = AdamW")
    args = parser.parse_args()
    
    # Ki·ªÉm tra logic tham s·ªë
    if args.mode in ["pretrain", "evaluate"] and not args.checkpoint:
        parser.error(f"--checkpoint l√† b·∫Øt bu·ªôc khi mode l√† '{args.mode}'")
        
    return args
def set_seed():
    torch.manual_seed(SEED)
    random.seed(SEED)
    np.random.seed(SEED)
    torch.cuda.manual_seed(SEED)
    torch.cuda.manual_seed_all(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
# --- [TH√äM H√ÄM N√ÄY] H√ÄM H·ªñ TR·ª¢ ƒê√ìNG/M·ªû BƒÇNG ---
def set_grad_status(model, freeze=True):
    """
    H√†m ƒë√≥ng bƒÉng ho·∫∑c m·ªü bƒÉng Backbone/Encoder.
    H·ªó tr·ª£ c·∫£ Model Custom (self.backbone) v√† Model SMP (self.encoder).
    """
    target_module = None
    
    # 1. Ki·ªÉm tra n·∫øu l√† Model Custom (PyramidCbamGateResNetUNet)
    if hasattr(model, 'backbone'):
        target_module = model.backbone
        name = "Backbone (ResNet)"
    # 2. Ki·ªÉm tra n·∫øu l√† Model SMP (DeepLabV3+, Unet++, ...)
    elif hasattr(model, 'encoder'):
        target_module = model.encoder
        name = "Encoder (SMP)"
    
    if target_module:
        for param in target_module.parameters():
            param.requires_grad = not freeze # Freeze = True -> requires_grad = False
        
        status = "FROZEN ‚ùÑÔ∏è" if freeze else "UNFROZEN üî•"
        print(f"[INFO] {name} is now {status}")
    else:
        print("[WARNING] Could not find 'backbone' or 'encoder' to freeze!")
def main(args):  
    print(f"\n[DEBUG TRAIN] args.loss b·∫°n nh·∫≠p t·ª´ b√†n ph√≠m = {args.loss}")
    print("-" * 50)
    import numpy as np    
    from trainer import Trainer
    from model import Unet, unet_pyramid_cbam_gate, Swin_unet
    # from model import Swin_unet
    import optimizer as optimizer_module
    from dataset import get_dataloaders
    from result import export, export_evaluate
    global trainer
    from utils import get_loss_instance, _focal_tversky_global
    from torch.optim.swa_utils import AveragedModel, SWALR, update_bn
    import shutil
    # from utils import loss_func
    from torch.optim.lr_scheduler import _LRScheduler
    print("-" * 50)
    print(f"[INFO] Mode: {args.mode.upper()}")
    print("-" * 50)

    set_seed()
    
    # 1. Kh·ªüi t·∫°o Model
    print(f"[INFO] Initializing Model...")
    # model = unet_pyramid_cbam_gate.PyramidCbamGateResNet50UNet(
    #     in_channels=3, 
    #     out_channels=1, 
    #     deep_supervision=True,
    #     dropout_prob=0.5)
    # model = smp.DeepLabV3Plus(
    #         # encoder_name="tu-resnest50d", # ResNeSt r·∫•t m·∫°nh cho y t·∫ø, ho·∫∑c d√πng efficientnet-b3
    #         # encoder_name = "efficientnet-b4"
    #         encoder_name="tu-resnest50d",
    #         encoder_weights="imagenet",
    #         in_channels=3,
    #         classes=1,
    #         drop_path_rate=0.2
    # )
    # UNET++ (used)
#     model = smp.UnetPlusPlus(
#         encoder_name="tu-resnest50d", 
#         encoder_weights="imagenet",
#         in_channels=3,
#         classes=1,
#         drop_path_rate=0.5
# )
    # SwinUnet => Size ·∫£nh ko kh·ªõp => B·ªè
    
#     model = smp.Unet(
#         # --- C·∫§U H√åNH QUAN TR·ªåNG NH·∫§T ---
#         # Thay backbone CNN (ResNest) b·∫±ng Swin Transformer
#         # C√°c l·ª±a ch·ªçn: 
#         # - 'swin_tiny_patch4_window7_224' (Nh·∫π nh·∫•t, ~ResNet50)
#         # - 'swin_small_patch4_window7_224' (~ResNet101)
#         # - 'swin_base_patch4_window7_224' (M·∫°nh, r·∫•t n·∫∑ng VRAM)
#         encoder_name="tu-swin_base_patch4_window7_224", 
        
#         encoder_weights="imagenet",
#         in_channels=3,
#         classes=1,
        
#         # Swin Transformer kh√¥ng nh·∫≠n drop_path_rate ·ªü ngo√†i
#         # Ph·∫£i ƒë∆∞a v√†o encoder_params
#         encoder_params={
#             "drop_path_rate": 0.2, # Swin kh√° nh·∫°y c·∫£m, n√™n ƒë·ªÉ th·∫•p (0.2-0.3)
#         },
        
#         # T√πy ch·ªçn: Th√™m Attention cho Decoder ƒë·ªÉ "full option"
#         decoder_attention_type="scse" 
# )
    #  Thay th·∫ø SwinUnet b·∫±ng ConvNeXt
# -------------BO DROP-----------------------
#     model = smp.Unet(
#         # ConvNeXt Tiny: M·∫°nh ~ ResNet50 / Swin-Tiny
#         # ConvNeXt Base: M·∫°nh ~ ResNet101 / Swin-Base
#         # Th√™m ti·ªÅn t·ªë "tu-" v√¨ n√≥ l·∫•y t·ª´ timm
#         encoder_name="tu-convnext_tiny", 
        
#         encoder_weights="imagenet",
#         in_channels=3,
#         classes=1,
        
#         # ConvNeXt d√πng Drop Path gi·ªëng Swin
#         drop_path_rate=0.2,
#         # V·∫´n n√™n th√™m Attention cho Decoder
#         decoder_attention_type="scse"
# )
# -------------BO DROP-----------------------
    #  Thay th·∫ø SwinUnet b·∫±ng ConvNeX
    def set_drop_path_rate(model, drop_rate=0.2):
        count = 0
        # Duy·ªát qua t·∫•t c·∫£ c√°c module trong encoder
        for module in model.encoder.modules():
            # Ki·ªÉm tra xem module c√≥ ph·∫£i l√† DropPath c·ªßa timm kh√¥ng
            # (Th∆∞·ªùng t√™n class s·∫Ω ch·ª©a ch·ªØ 'DropPath')
            if "DropPath" in module.__class__.__name__:
                module.drop_prob = drop_rate
                count += 1
        print(f"[INFO] ƒê√£ c·∫≠p nh·∫≠t DropPath rate = {drop_rate} cho {count} blocks trong Encoder.")

    # G·ªçi h√†m ƒë·ªÉ set rate l√† 0.2
    set_drop_path_rate(model, drop_rate=0.2)
    # tranUnet (using)
    # Thay v√¨ TransUNet (ch∆∞a c√≥ trong SMP), ta d√πng Unet v·ªõi Encoder l√† Transformer
#     model = smp.Unet(
#         # mit_b3 l√† backbone c·ªßa SegFormer, m·∫°nh t∆∞∆°ng ƒë∆∞∆°ng ResNet50/101
#         # nh∆∞ng d√πng c∆° ch·∫ø Self-Attention.
#         encoder_name="mit_b3",        
#         encoder_weights="imagenet",
#         in_channels=3,
#         classes=1,
#         # C√°c backbone Transformer trong SMP th∆∞·ªùng kh√¥ng nh·∫≠n tham s·ªë drop_path_rate 
#         # tr·ª±c ti·∫øp ·ªü ƒë√¢y, n√™n ta b·ªè d√≤ng ƒë√≥ ƒëi ƒë·ªÉ tr√°nh l·ªói.
#         decoder_use_batchnorm=True,
# )
    # UNET++ attention (Used)
#     model = smp.UnetPlusPlus(
#         encoder_name="tu-resnest50d", 
#         encoder_weights="imagenet",
#         in_channels=3,
#         classes=1,
        
#         # --- QUAN TR·ªåNG: TH√äM D√íNG N√ÄY ƒê·ªÇ C√ì ATTENTION ---
#         # scse gi√∫p m√¥ h√¨nh v·ª´a l·ªçc kh√¥ng gian (Spatial) v·ª´a l·ªçc k√™nh (Channel)
#         decoder_attention_type="scse",
        
#         # --- S·ª¨A L·ªñI DROP_PATH_RATE ---
#         # ƒê∆∞a v√†o encoder_params m·ªõi ƒë√∫ng c√∫ ph√°p
#         drop_path_rate=0.5
# )
    # UNet th∆∞·ªùng (Used)
#     model = smp.Unet(
#         encoder_name="tu-resnest50d", 
#         encoder_weights="imagenet",
#         in_channels=3,
#         classes=1,
#         # drop_path_rate n√™n ƒë∆∞·ª£c ƒë∆∞a v√†o encoder_params ƒë·ªÉ truy·ªÅn xu·ªëng backbone timm
#         drop_path_rate=0.5
# )
    # attentionUnet (Using)
#     model = smp.Unet(
#         encoder_name="tu-resnest50d", 
#         encoder_weights="imagenet",
#         in_channels=3,
#         classes=1,
        
#         # --- TH√äM D√íNG N√ÄY ƒê·ªÇ TH√ÄNH ATTENTION UNET ---
#         # scse: Spatial and Channel Squeeze & Excitation Attention
#         # N√≥ s·∫Ω ch√®n c√°c block attention v√†o sau m·ªói t·∫ßng Decoder
#         decoder_attention_type="scse",
        
#         # --- S·ª¨A L·ªñI DROP_PATH_RATE ---
#         # Ph·∫£i ƒë∆∞a v√†o encoder_params m·ªõi ƒë√∫ng, ƒë·ªÉ ·ªü ngo√†i s·∫Ω kh√¥ng c√≥ t√°c d·ª•ng ho·∫∑c b√°o l·ªói
#         drop_path_rate=0.5
# )
    # Segformer (Using)
#     model = smp.Segformer(
#         # Encoder chu·∫©n c·ªßa SegFormer l√† d√≤ng MiT (Mix Transformer)
#         # mit_b0 (nh·∫π nh·∫•t) -> mit_b5 (n·∫∑ng nh·∫•t)
#         # mit_b3 l√† l·ª±a ch·ªçn c√¢n b·∫±ng, m·∫°nh t∆∞∆°ng ƒë∆∞∆°ng ResNet50/ResNest50d
#         encoder_name="mit_b3",        
        
#         encoder_weights="imagenet",
#         in_channels=3,
#         classes=1,
        
#         # Encoder params v·∫´n d√πng ƒë·ªÉ truy·ªÅn drop_path_rate
#         # L∆∞u √Ω: V·ªõi Transformer, drop_path_rate th∆∞·ªùng ƒë·ªÉ th·∫•p (0.1) thay v√¨ 0.5
#         encoder_params={"drop_path_rate": 0.1} 
# )
    # 2. Kh·ªüi t·∫°o Optimizer
    opt = optimizer_module.optimizer(model=model) 
    # --- [CH√çNH X√ÅC: KH·ªûI T·∫†O SEQUENTIAL LR T·∫†I ƒê√ÇY] ---
    warmup_epochs = args.warmup if args.warmup > 0 else 10
    scheduler_warmup = LinearLR(
        opt, start_factor=0.01, end_factor=1.0, total_iters=warmup_epochs
    )
    # B. Main Cosine
    scheduler_cosine = CosineAnnealingWarmRestarts(
        opt, T_0=10, T_mult=2, eta_min=1e-6
    )
    # C. H·ª£p th·ªÉ (D√πng cho Giai ƒëo·∫°n 1 & 2)
    scheduler_initial = SequentialLR(
        opt, 
        schedulers=[scheduler_warmup, scheduler_cosine], 
        milestones=[warmup_epochs] 
    )
    # 3. KH·ªûI T·∫†O LOSS (Thay th·∫ø h√†m get_loss_function)
    # Logic: N·∫øu ch·ªçn FocalTversky th√¨ l·∫•y bi·∫øn to√†n c·ª•c, c√≤n l·∫°i th√¨ kh·ªüi t·∫°o class
    criterion_init = get_loss_instance(args.loss)
    # 4. Kh·ªüi t·∫°o Trainer
    # L∆∞u √Ω: Trainer l∆∞u reference t·ªõi criterion_init. 
    # N·∫øu criterion_init l√† _focal_tversky_loss, m·ªçi thay ƒë·ªïi tr√™n _focal_tversky_loss s·∫Ω t·ª± ƒë·ªông c·∫≠p nh·∫≠t trong Trainer.
    trainer = Trainer(model=model, optimizer=opt, criterion=criterion_init, scheduler=scheduler_initial, patience=10, device=DEVICE)

    if args.mode == "train":
        if not os.path.exists(BASE_OUTPUT):
            os.makedirs(BASE_OUTPUT)
        # resume_checkpoint = None
        if args.augment:
            mode_stage1 = 'weak'
            mode_stage23 = 'strong'
            print(f"[INFO] Augmentation: ON (Stage 1: {mode_stage1} -> Stage 2/3: {mode_stage23})")
        else:
            mode_stage1 = 'none'
            mode_stage23 = 'none'
            print(f"[INFO] Augmentation: OFF (All Stages: none)")
        # if args.augment and args.warmup > 0:
        if args.warmup > 0:

            # =========================================================
            # GIAI ƒêO·∫†N 1: WARM-UP
            # =========================================================
            print("\n" + "="*40)
            print(" GIAI ƒêO·∫†N 1: WARM-UP (Freeze Backbone) (10 Epochs)")
            print(f" Config: Light Augment | Loss: {args.loss}")
            print("="*40)

            trainLoader_weak, validLoader, _ = get_dataloaders(aug_mode=mode_stage1)
            
            # ƒê·∫£m b·∫£o params ƒë√∫ng cho GD1 (n·∫øu d√πng Focal)
            if args.loss == "FocalTversky_loss":
                _focal_tversky_global.update_params(alpha=0.7, beta=0.3, gamma=1.33)
            # --- [TH√äM] ƒê√ìNG BƒÇNG BACKBONE ---
            set_grad_status(model, freeze=True)
            trainer.num_epochs = args.warmup
            trainer.patience = 999      
            trainer.train(trainLoader_weak, validLoader, resume_path=None)
            # --- [TH√äM] M·ªû BƒÇNG BACKBONE (ƒê·ªÉ chu·∫©n b·ªã cho GD2) ---
            set_grad_status(model, freeze=False)
            resume_checkpoint = "best_dice_mass_model.pth" 
        else:
            print("\n[INFO] Skipping Stage 1 (Warm-up). Starting directly with Main Training.")
            # ƒê·∫£m b·∫£o ch·∫Øc ch·∫Øn l√† ƒë√£ Unfreeze n·∫øu kh√¥ng ch·∫°y Stage 1
            set_grad_status(model, freeze=False)
            resume_checkpoint = None
        # =========================================================
        # GIAI ƒêO·∫†N 2: INTERMEDIATE TUNING (Ch·ªâ FocalTversky)
        # =========================================================
        if args.loss == "FocalTversky_loss":
            print("\n" + "="*40)
            print(" GIAI ƒêO·∫†N 2: INTERMEDIATE TUNING (Full Finetune)")
            print(" Config: Heavy Augment | Alpha=0.7")
            print("="*40)
            # ƒê·∫£m b·∫£o backbone ƒë√£ ƒë∆∞·ª£c m·ªü kh√≥a (double check)
            set_grad_status(model, freeze=False)
            trainLoader_strong, validLoader, _ = get_dataloaders(aug_mode=mode_stage23)
            
            # Update Params (Th·ª±c t·∫ø GD2 v·∫´n d√πng 0.7, nh∆∞ng g·ªçi l·∫°i cho ch·∫Øc ch·∫Øn ho·∫∑c n·∫øu b·∫°n mu·ªën ch·ªânh kh√°c)
            # KH√îNG C·∫¶N g√°n trainer.criterion = ... v√¨ n√≥ ƒë√£ tr·ªè c√πng 1 v√πng nh·ªõ
            _focal_tversky_global.update_params(alpha=0.7, beta=0.3, gamma=1.33)
            
            trainer.num_epochs = 150 
            trainer.patience = 10   
            trainer.early_stop_counter = 0
            
            trainer.train(trainLoader_strong, validLoader, resume_path=resume_checkpoint)
            resume_checkpoint = "best_dice_mass_model.pth"
            print(f"[TRANSITION] Stage 2 Finished. Best model '{resume_checkpoint}' will be loaded for Stage 3.")
            if os.path.exists(resume_checkpoint):
                backup_name = "stage2_final_best.pth" # T√™n file backup
                shutil.copy(resume_checkpoint, backup_name)
                print(f"[BACKUP] Safe copy created: {resume_checkpoint} -> {backup_name}")
        # ----------------------------------------
        else:
            print("\n[INFO] Skipping Stage 2 (Only for FocalTversky).")

        # =========================================================
        # GIAI ƒêO·∫†N 3: FINAL TRAINING
        # =========================================================
        # print("\n[INFO] MANUAL RESUME: Skipping Stage 2.")
        # resume_checkpoint = "best_dice_mass_model.pth"
        # if os.path.exists(resume_checkpoint):
        #      print(f"[INFO] Found Stage 2 Checkpoint: {resume_checkpoint}. Proceeding to Stage 3.")
        # else:
        #      print(f"[ERROR] Checkpoint {resume_checkpoint} not found! Check file name.")
        #      return # D·ª´ng ch∆∞∆°ng tr√¨nh n·∫øu kh√¥ng th·∫•y file
        print("\n" + "="*40)
        print(" GIAI ƒêO·∫†N 3: FINAL TRAINING")
        # [B∆Ø·ªöC 1: QUAN TR·ªåNG] Load Checkpoint th·ªß c√¥ng TR∆Ø·ªöC KHI ch·ªânh s·ª≠a b·∫•t c·ª© th·ª© g√¨
        if resume_checkpoint and os.path.exists(resume_checkpoint):
            print(f"[INFO] Manually loading checkpoint for Stage 3 setup: {resume_checkpoint}")
            trainer.load_checkpoint(resume_checkpoint)
        else:
            print("[WARNING] No checkpoint found for Stage 3! Training from scratch?")
            
        set_grad_status(model, freeze=False)
        if args.loss == "FocalTversky_loss":
            # >> CHI·∫æN L∆Ø·ª¢C 3 GIAI ƒêO·∫†N (Focal) <<
            print(" Config: Heavy Augment | Alpha=0.4 (Reduce FP) | LR REDUCED Strategy: Start Low (1e-5) -> Restart High (1e-4)")
            
            # 1. Update params "n√≥ng"
            _focal_tversky_global.update_params(alpha=0.4, beta=0.6, gamma=1.33)
            
            # 2. Reset Best Loss (V√¨ scale loss thay ƒë·ªïi)
            trainer.best_val_loss = float('inf')
            # 3. Gi·∫£m LR
            # current_lr = trainer.optimizer.param_groups[0]['lr']
            # new_lr = current_lr * 0.1
            new_lr = 1e-5
            for param_group in trainer.optimizer.param_groups:
                param_group['lr'] = new_lr
            print(f"[SWITCH] Switching logic from SequentialLR -> ReduceLROnPlateau for Final Stage with LR forced to {new_lr}")
            
            # # # C·∫≠p nh·∫≠t "tr·∫ßn" cho Scheduler ƒë·ªÉ c√°c chu k·ª≥ sau kh√¥ng v∆∞·ª£t qu√° 1e-5
            # if hasattr(trainer.scheduler, 'base_lrs'):
            #      trainer.scheduler.base_lrs = [new_lr] * len(trainer.optimizer.param_groups)

            # print(f"[CONFIG] Scheduler continued! New Peak LR set to: {new_lr}")
            
            # 4. KH·ªûI T·∫†O L·∫†I SCHEDULER (Hack th·ªùi gian)
            CYCLE_START = 10
            CYCLE_ADD = 10
            # fake_last_epoch = 7  # M·∫πo: Gi·∫£ v·ªù l√† ƒë√£ ch·∫°y ƒë∆∞·ª£c 7 epoch -> ƒêang ·ªü g·∫ßn ƒë√°y chu k·ª≥

            trainer.scheduler = ReduceLROnPlateau(
                trainer.optimizer, 
                mode='max',      # Theo d√µi Dice Mass (c√†ng cao c√†ng t·ªët)
                factor=0.5,      # Gi·∫£m 1 n·ª≠a khi b√£o h√≤a
                patience=5,     # Ch·ªù 10 epoch
                # verbose=True,
                min_lr=1e-6      # ƒê√°y ƒë·ªÉ k√≠ch ho·∫°t reset
            )
            print(f"[CONFIG] Scheduler Reset! Mode: Arithmetic (10 -> 20 -> 30...)")            
        else:
            # >> CHI·∫æN L∆Ø·ª¢C 2 GIAI ƒêO·∫†N (Loss kh√°c) <<
            print(f" Config: Heavy Augment | Loss: {args.loss} | KEEP LR")
            # Kh√¥ng gi·∫£m LR, Kh√¥ng ƒë·ªïi params
            # Trainer v·∫´n gi·ªØ nguy√™n criterion kh·ªüi t·∫°o t·ª´ ƒë·∫ßu

        print("="*40)

        # Load Data Strong (D√πng chung cho c·∫£ 2 nh√°nh)
        trainLoader_strong, validLoader, _ = get_dataloaders(aug_mode=mode_stage23)
        
        trainer.num_epochs = NUM_EPOCHS # Max epoch
        trainer.patience = 25  # Patient 20 cho GD3         
        trainer.early_stop_counter = 0
        # Ch·∫°y GD3 ƒë·∫øn khi Early Stop k√≠ch ho·∫°t        
        trainer.train(trainLoader_strong, validLoader, resume_path=None) 
        print("\n[INFO] Exporting Main Training Results (Stage 1-3)...")
        export(trainer)
        # =========================================================
        # GIAI ƒêO·∫†N 4: SWA (STOCHASTIC WEIGHT AVERAGING)
        # =========================================================
        # Ch·ªâ ch·∫°y SWA n·∫øu ƒëang d√πng FocalTversky (chi·∫øn l∆∞·ª£c c·ªßa b·∫°n)
        # if args.loss == "FocalTversky_loss":
        #     print("\n" + "="*40)
        #     print(" GIAI ƒêO·∫†N 4: SWA FINETUNING (The Secret Weapon)")
        #     print(" Strategy: Constant LR | No Early Stop | 20 Epochs")
        #     print("="*40)

        #     # 1. QUAN TR·ªåNG: Load l·∫°i BEST MODEL c·ªßa GD3 (Kh√¥ng d√πng model cu·ªëi c√πng)
        #     # best_model_path = "best_dice_mass_model.pth"
        #     best_ep = trainer.best_epoch_dice
        #     best_d = trainer.best_dice_mass
        #     folder_name = f"output_epoch{best_ep}_diceMass{best_d:.4f}"
        #     exported_best_model_path = os.path.join(BASE_OUTPUT, folder_name, "best_dice_mass_model.pth")
        #     if os.path.exists(exported_best_model_path):
        #         print(f"[INFO] Loading BEST model from Stage 3 for SWA: {exported_best_model_path}")
        #         trainer.load_checkpoint(exported_best_model_path)
        #     else:
        #         print("[WARNING] Could not find exported best model. Trying local 'best_dice_mass_model.pth'...")
        #         if os.path.exists("best_dice_mass_model.pth"):
        #             trainer.load_checkpoint("best_dice_mass_model.pth")

        #     # 2. Kh·ªüi t·∫°o SWA
        #     swa_model = AveragedModel(trainer.model)
        #     # LR cho SWA: Cao h∆°n GD3 m·ªôt ch√∫t ƒë·ªÉ tho√°t h·ªë (5e-5 l√† an to√†n v·ªõi AdamW)
        #     swa_lr = 5e-5 
        #     swa_scheduler = SWALR(trainer.optimizer, swa_lr=swa_lr, anneal_epochs=3)
            
        #     print(f"[CONFIG] SWA Scheduler set. LR: {swa_lr}")

        #     # 3. C·∫•u h√¨nh v√≤ng l·∫∑p SWA
        #     SWA_EPOCHS = 5 # Ch·∫°y c·ªë ƒë·ªãnh
        #     trainer.patience = 999 # T·∫Øt Early Stop
        #     trainer.early_stop_counter = 0
            
        #     # Ch√∫ng ta s·∫Ω d√πng l·∫°i h√†m train() c·ªßa Trainer nh∆∞ng ch·∫°y t·ª´ng epoch m·ªôt
        #     # ƒë·ªÉ ch√®n logic update_parameters v√†o gi·ªØa.
            
        #     print("[INFO] Starting SWA Loop...")
        #     for epoch in range(SWA_EPOCHS):
        #         # Hack: Set epoch = 1 ƒë·ªÉ Trainer ch·∫°y 1 v√≤ng r·ªìi tho√°t ra
        #         trainer.num_epochs = 1 
        #         trainer.start_epoch = 0 
        #         # G√°n scheduler SWA v√†o trainer
        #         trainer.scheduler = swa_scheduler
                
        #         # Train 1 epoch (Kh√¥ng load checkpoint, ch·∫°y ti·∫øp t·ª´ b·ªô nh·ªõ)
        #         # L∆∞u √Ω: Trainer s·∫Ω in ra log validation, c·ª© k·ªá n√≥.
        #         print(f"\n[SWA] Epoch {epoch+1}/{SWA_EPOCHS}")
        #         trainer.train(trainLoader_strong, validLoader, resume_path=None)
                
        #         # C·∫≠p nh·∫≠t tr·ªçng s·ªë trung b√¨nh
        #         swa_model.update_parameters(trainer.model)
                
        #         # Step Scheduler
        #         swa_scheduler.step()
                
        #     # 4. C·∫≠p nh·∫≠t Batch Norm (B∆∞·ªõc b·∫Øt bu·ªôc)
        #     print("\n[INFO] Updating Batch Normalization statistics for SWA Model...")
        #     update_bn(trainLoader_strong, swa_model, device=DEVICE)

        #     # 5. L∆∞u v√† ƒê√°nh gi√° SWA Model
        #     swa_save_path = os.path.join(BASE_OUTPUT, "best_model_swa.pth")
        #     print(f"[INFO] Saving SWA Model to {swa_save_path}")
        #     swa_checkpoint = {
        #         'epoch': SWA_EPOCHS,
        #         'model_state_dict': swa_model.state_dict(),         # <--- ƒê√£ s·ª≠a ƒë·ªÉ kh·ªõp t√™n layer
        #         'optimizer_state_dict': trainer.optimizer.state_dict(), # ƒê·ªÉ kh√¥ng l·ªói optimizer
                
        #         # C√°c ch·ªâ s·ªë th·ªëng k√™ (L·∫•y t·ª´ trainer hi·ªán t·∫°i ƒë·ªÉ l∆∞u l√†m k·ª∑ ni·ªám)
        #         'best_val_loss': trainer.best_val_loss, 
        #         'best_dice_mass': trainer.best_dice_mass,
        #         'best_iou_mass': trainer.best_iou_mass,
        #         # 'history': trainer.history,
                
        #         # QUAN TR·ªåNG: KH√îNG ƒê∆Ø·ª¢C TH√äM 'scheduler_state_dict' V√ÄO ƒê√ÇY
        #         # N·∫øu th√™m 'scheduler_state_dict': None -> S·∫Ω b·ªã l·ªói NoneType crash ngay.
        #     }
        #     torch.save(swa_checkpoint, swa_save_path)
        #     # export(trainer)
        #     # ƒê√°nh gi√° Model SWA
        #     print("\n[INFO] Evaluating SWA Model...")
        #     # G√°n model SWA v√†o trainer ƒë·ªÉ evaluate
        #     trainer.model = swa_model
            
        #     visual_folder = os.path.join(BASE_OUTPUT, "prediction_images_swa")
        #     os.makedirs(visual_folder, exist_ok=True)
            
        #     trainer.evaluate(
        #         test_loader=validLoader, 
        #         checkpoint_path=swa_save_path,
        #         save_visuals=True,          
        #         output_dir=visual_folder    
        #     )
        #     export_evaluate(trainer, split_name="valid_swa")
            
    # (Gi·ªØ nguy√™n ph·∫ßn pretrain/evaluate)
    elif args.mode == "pretrain":
        aug_type = 'strong' if args.augment else 'none'
        trainLoader, validLoader, _ = get_dataloaders(aug_mode=aug_type)
        trainer.patience = 20
        trainer.train(trainLoader, validLoader, resume_path=args.checkpoint)
        export(trainer)
    elif args.mode == "evaluate":
        print(f"[INFO] Mode: EVALUATING FULL DATASET")
        
        trainLoader, validLoader, testLoader = get_dataloaders(aug_mode='none', state='evaluate')
        
        eval_tasks = [
            # (trainLoader, "train"),
            (validLoader, "valid"),
            (testLoader, "test")
        ]
        
        for loader, split_name in eval_tasks:
            print(f"\n" + "="*40)
            print(f" [EVALUATING] Processing: {split_name.upper()} SET")
            print("="*40)
            
            visual_folder = os.path.join(BASE_OUTPUT, f"prediction_images_{split_name}")
            if not os.path.exists(visual_folder):
                os.makedirs(visual_folder)
            
            trainer.evaluate(
                test_loader=loader, 
                checkpoint_path=args.checkpoint,
                save_visuals=True,          
                output_dir=visual_folder    
            )
            
            # --- G·ªåI H√ÄM V·ªöI THAM S·ªê M·ªöI ---
            print(f"[INFO] Exporting metrics for {split_name}...")
            export_evaluate(trainer, split_name=split_name)

if __name__ == "__main__":
    args = get_args()
    main(args)
